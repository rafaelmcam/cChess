{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cChess.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "fEVhnCGpeCHB",
        "colab_type": "code",
        "outputId": "6568d21b-8a20-483c-bcba-5f1d79449eb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cChess'...\n",
            "remote: Enumerating objects: 3601, done.\u001b[K\n",
            "remote: Counting objects: 100% (3601/3601), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3573/3573), done.\u001b[K\n",
            "remote: Total 3601 (delta 63), reused 3563 (delta 28), pack-reused 0\n",
            "Receiving objects: 100% (3601/3601), 15.00 MiB | 17.22 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iBet8Z0URdwH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir preview"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4XZCIGmFPjEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75469310-6ef4-4467-e060-600e1889d40e"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "img = load_img('cChess/Labels/B/0_0_72.png')  # this is a PIL image\n",
        "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
        "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
        "\n",
        "# the .flow() command below generates batches of randomly transformed images\n",
        "# and saves the results to the `preview/` directory\n",
        "i = 0\n",
        "for batch in datagen.flow(x, batch_size=1,\n",
        "                          save_to_dir='preview', save_prefix='bishop', save_format='jpeg'):\n",
        "    i += 1\n",
        "    if i > 20:\n",
        "        break  # otherwise the generator would loop indefinitely"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "jy_5mKWlR5cR",
        "colab_type": "code",
        "outputId": "35a511a9-c412-4d81-b898-00cc02a7e0a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x.shape\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 50, 50, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "DCXoGCabRc4N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "4WDUrdBAUrPz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "756fc690-1229-4731-efea-fac683566801"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(50,50,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (1, 1)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(13))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_27 (Conv2D)           (None, 48, 48, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 22, 22, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 22, 22, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 9, 9, 64)          18496     \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 2, 2, 256)         147712    \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 1, 1, 256)         65792     \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 13)                1677      \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 13)                0         \n",
            "=================================================================\n",
            "Total params: 276,717\n",
            "Trainable params: 276,717\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WUO4PVXyVB1O",
        "colab_type": "code",
        "outputId": "94028006-7f63-4bb5-9fd9-b0f8ed227e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# this is the augmentation configuration we will use for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "\n",
        "# this is a generator that will read pictures found in\n",
        "# subfolers of 'data/train', and indefinitely generate\n",
        "# batches of augmented image data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'cChess/Labels',  # this is the target directory\n",
        "        target_size=(50, 50),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3392 images belonging to 13 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XvGBITCWVdt2",
        "colab_type": "code",
        "outputId": "5904ce03-7197-407d-ae0b-98cce03cb69e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=3392 // batch_size,\n",
        "        epochs=50)\n",
        "model.save_weights('/cChess/Weights/first_try.h5')  # always save your weights after training or during training"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 1.9159 - acc: 0.4920\n",
            "Epoch 2/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 1.4904 - acc: 0.5985\n",
            "Epoch 3/50\n",
            "53/53 [==============================] - 3s 65ms/step - loss: 1.2111 - acc: 0.6362\n",
            "Epoch 4/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.9305 - acc: 0.7008\n",
            "Epoch 5/50\n",
            "53/53 [==============================] - 3s 65ms/step - loss: 0.8539 - acc: 0.7249\n",
            "Epoch 6/50\n",
            "53/53 [==============================] - 3s 65ms/step - loss: 0.7565 - acc: 0.7423\n",
            "Epoch 7/50\n",
            "53/53 [==============================] - 3s 65ms/step - loss: 0.6330 - acc: 0.7792\n",
            "Epoch 8/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.5830 - acc: 0.7904\n",
            "Epoch 9/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.5326 - acc: 0.8031\n",
            "Epoch 10/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.7214 - acc: 0.7692\n",
            "Epoch 11/50\n",
            "53/53 [==============================] - 3s 65ms/step - loss: 0.5066 - acc: 0.8169\n",
            "Epoch 12/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.4574 - acc: 0.8302\n",
            "Epoch 13/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.4121 - acc: 0.8491\n",
            "Epoch 14/50\n",
            "53/53 [==============================] - 3s 65ms/step - loss: 0.3889 - acc: 0.8629\n",
            "Epoch 15/50\n",
            "53/53 [==============================] - 3s 65ms/step - loss: 0.3653 - acc: 0.8626\n",
            "Epoch 16/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.3413 - acc: 0.8800\n",
            "Epoch 17/50\n",
            "53/53 [==============================] - 3s 65ms/step - loss: 0.3689 - acc: 0.8738\n",
            "Epoch 18/50\n",
            "53/53 [==============================] - 4s 73ms/step - loss: 0.3333 - acc: 0.8800\n",
            "Epoch 19/50\n",
            "53/53 [==============================] - 4s 73ms/step - loss: 0.2854 - acc: 0.9009\n",
            "Epoch 20/50\n",
            "53/53 [==============================] - 4s 70ms/step - loss: 0.2567 - acc: 0.9065\n",
            "Epoch 21/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.2383 - acc: 0.9148\n",
            "Epoch 22/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.2344 - acc: 0.9213\n",
            "Epoch 23/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.2227 - acc: 0.9213\n",
            "Epoch 24/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.1880 - acc: 0.9337\n",
            "Epoch 25/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.1895 - acc: 0.9343\n",
            "Epoch 26/50\n",
            "53/53 [==============================] - 3s 65ms/step - loss: 0.1908 - acc: 0.9340\n",
            "Epoch 27/50\n",
            "53/53 [==============================] - 3s 63ms/step - loss: 0.1724 - acc: 0.9437\n",
            "Epoch 28/50\n",
            "53/53 [==============================] - 3s 63ms/step - loss: 0.1622 - acc: 0.9455\n",
            "Epoch 29/50\n",
            "53/53 [==============================] - 3s 63ms/step - loss: 0.1568 - acc: 0.9493\n",
            "Epoch 30/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.1438 - acc: 0.9531\n",
            "Epoch 31/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.1458 - acc: 0.9508\n",
            "Epoch 32/50\n",
            "53/53 [==============================] - 3s 63ms/step - loss: 0.1260 - acc: 0.9567\n",
            "Epoch 33/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.0969 - acc: 0.9688\n",
            "Epoch 34/50\n",
            "53/53 [==============================] - 3s 63ms/step - loss: 0.1130 - acc: 0.9646\n",
            "Epoch 35/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.1371 - acc: 0.9564\n",
            "Epoch 36/50\n",
            "53/53 [==============================] - 3s 63ms/step - loss: 0.1058 - acc: 0.9634\n",
            "Epoch 37/50\n",
            "53/53 [==============================] - 3s 63ms/step - loss: 0.0877 - acc: 0.9726\n",
            "Epoch 38/50\n",
            "53/53 [==============================] - 3s 63ms/step - loss: 0.0789 - acc: 0.9746\n",
            "Epoch 39/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.0688 - acc: 0.9773\n",
            "Epoch 40/50\n",
            "53/53 [==============================] - 3s 63ms/step - loss: 0.0678 - acc: 0.9773\n",
            "Epoch 41/50\n",
            "53/53 [==============================] - 4s 68ms/step - loss: 0.0783 - acc: 0.9752\n",
            "Epoch 42/50\n",
            "53/53 [==============================] - 4s 73ms/step - loss: 0.0965 - acc: 0.9679\n",
            "Epoch 43/50\n",
            "53/53 [==============================] - 4s 73ms/step - loss: 0.0680 - acc: 0.9797\n",
            "Epoch 44/50\n",
            "53/53 [==============================] - 3s 64ms/step - loss: 0.0786 - acc: 0.9761\n",
            "Epoch 45/50\n",
            "53/53 [==============================] - 3s 63ms/step - loss: 0.0911 - acc: 0.9717\n",
            "Epoch 46/50\n",
            "53/53 [==============================] - 3s 63ms/step - loss: 0.0637 - acc: 0.9805\n",
            "Epoch 47/50\n",
            "53/53 [==============================] - 3s 63ms/step - loss: 0.0772 - acc: 0.9782\n",
            "Epoch 48/50\n",
            "53/53 [==============================] - 4s 68ms/step - loss: 0.0504 - acc: 0.9835\n",
            "Epoch 49/50\n",
            "53/53 [==============================] - 4s 73ms/step - loss: 0.0581 - acc: 0.9838\n",
            "Epoch 50/50\n",
            "53/53 [==============================] - 3s 63ms/step - loss: 0.0664 - acc: 0.9776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wDnJCBxMLFsH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!\\cd cChess && mkdir Weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0SA4oriDetNy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gC14LZSWWIPr",
        "colab_type": "code",
        "outputId": "e31c007c-f0fe-413a-cf24-c3bf03bf2b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-b5585119e305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mtop_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mtop_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtop_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mtop_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mtop_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m             if all([s is not None\n\u001b[1;32m    473\u001b[0m                     for s in to_list(input_shape)]):\n\u001b[0;32m--> 474\u001b[0;31m                 \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    498\u001b[0m             raise ValueError('The shape of the input to \"Flatten\" '\n\u001b[1;32m    499\u001b[0m                              \u001b[0;34m'is not fully defined '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m                              \u001b[0;34m'(got '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m                              \u001b[0;34m'Make sure to pass a complete \"input_shape\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                              \u001b[0;34m'or \"batch_input_shape\" argument to the first '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The shape of the input to \"Flatten\" is not fully defined (got (None, None, 512). Make sure to pass a complete \"input_shape\" or \"batch_input_shape\" argument to the first layer in your model."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "sfgteWLCg5Xa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7ad8652-7256-4641-8a4c-f8a03ef5a6ba"
      },
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'cChess/Labels',  # this is the target directory\n",
        "        target_size=(50, 50),  # all images will be resized to 150x150\n",
        "        batch_size=10000,\n",
        "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
        "\n",
        "x,y = train_generator.next()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3392 images belonging to 13 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CRxtzbaGKLlo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "866b0b1d-d8a4-45b6-c43a-2b9c6161376c"
      },
      "cell_type": "code",
      "source": [
        "x.shape\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3392, 50, 50, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "-ULBSQ8dKNs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89e71186-6283-4460-c7a9-ab02482fb48e"
      },
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3392, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "aeGZ5LhRKO3Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3434
        },
        "outputId": "e86f4a9b-17a9-43c8-f943-81a3cd0625ba"
      },
      "cell_type": "code",
      "source": [
        "'''This script goes along the blog post\n",
        "\"Building powerful image classification models using very little data\"\n",
        "from blog.keras.io.\n",
        "It uses data that can be downloaded at:\n",
        "https://www.kaggle.com/c/dogs-vs-cats/data\n",
        "In our setup, we:\n",
        "- created a data/ folder\n",
        "- created train/ and validation/ subfolders inside data/\n",
        "- created cats/ and dogs/ subfolders inside train/ and validation/\n",
        "- put the cat pictures index 0-999 in data/train/cats\n",
        "- put the cat pictures index 1000-1400 in data/validation/cats\n",
        "- put the dogs pictures index 12500-13499 in data/train/dogs\n",
        "- put the dog pictures index 13500-13900 in data/validation/dogs\n",
        "So that we have 1000 training examples for each class, and 400 validation examples for each class.\n",
        "In summary, this is our directory structure:\n",
        "```\n",
        "data/\n",
        "    train/\n",
        "        dogs/\n",
        "            dog001.jpg\n",
        "            dog002.jpg\n",
        "            ...\n",
        "        cats/\n",
        "            cat001.jpg\n",
        "            cat002.jpg\n",
        "            ...\n",
        "    validation/\n",
        "        dogs/\n",
        "            dog001.jpg\n",
        "            dog002.jpg\n",
        "            ...\n",
        "        cats/\n",
        "            cat001.jpg\n",
        "            cat002.jpg\n",
        "            ...\n",
        "```\n",
        "'''\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from keras import applications\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 8\n",
        "\n",
        "\n",
        "def save_bottlebeck_features(x,y):\n",
        "\n",
        "    # build the VGG16 network\n",
        "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
        "    bottleneck_features_train = model.predict(\n",
        "        x)\n",
        "    print(bottleneck_features_train.shape)\n",
        "    np.save(open('./cChess/Weights/bottleneck_features_train.npy', 'wb'),\n",
        "            bottleneck_features_train)\n",
        "    np.save(open('./cChess/Weights/bottleneck_labels.npy', 'wb'),\n",
        "            y)\n",
        "\n",
        "def train_top_model():\n",
        "    train_data = np.load(open('./cChess/Weights/bottleneck_features_train.npy', \"rb\"))\n",
        "    train_labels = np.load(open('./cChess/Weights/bottleneck_labels.npy',\"rb\"))\n",
        "\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(13, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_labels,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_split=.2)\n",
        "    model.save_weights(\"./cChess/Weights/bottleneck512.h5\")\n",
        "\n",
        "\n",
        "#save_bottlebeck_features(x,y)\n",
        "train_top_model()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2713 samples, validate on 679 samples\n",
            "Epoch 1/100\n",
            "2713/2713 [==============================] - 2s 790us/step - loss: 1.2528 - acc: 0.6491 - val_loss: 0.8729 - val_acc: 0.7261\n",
            "Epoch 2/100\n",
            "2713/2713 [==============================] - 1s 426us/step - loss: 0.8192 - acc: 0.7512 - val_loss: 0.6635 - val_acc: 0.7909\n",
            "Epoch 3/100\n",
            "2713/2713 [==============================] - 1s 426us/step - loss: 0.6507 - acc: 0.8006 - val_loss: 0.5229 - val_acc: 0.8247\n",
            "Epoch 4/100\n",
            "2713/2713 [==============================] - 1s 427us/step - loss: 0.5523 - acc: 0.8268 - val_loss: 0.4773 - val_acc: 0.8483\n",
            "Epoch 5/100\n",
            "2713/2713 [==============================] - 1s 434us/step - loss: 0.5035 - acc: 0.8397 - val_loss: 0.4248 - val_acc: 0.8704\n",
            "Epoch 6/100\n",
            "2713/2713 [==============================] - 1s 425us/step - loss: 0.4871 - acc: 0.8400 - val_loss: 0.3563 - val_acc: 0.8866\n",
            "Epoch 7/100\n",
            "2713/2713 [==============================] - 1s 427us/step - loss: 0.4116 - acc: 0.8684 - val_loss: 0.3451 - val_acc: 0.8954\n",
            "Epoch 8/100\n",
            "2713/2713 [==============================] - 1s 430us/step - loss: 0.3829 - acc: 0.8750 - val_loss: 0.3340 - val_acc: 0.8984\n",
            "Epoch 9/100\n",
            "2713/2713 [==============================] - 1s 430us/step - loss: 0.3587 - acc: 0.8828 - val_loss: 0.3214 - val_acc: 0.8969\n",
            "Epoch 10/100\n",
            "2713/2713 [==============================] - 1s 475us/step - loss: 0.3528 - acc: 0.8791 - val_loss: 0.3032 - val_acc: 0.8984\n",
            "Epoch 11/100\n",
            "2713/2713 [==============================] - 1s 510us/step - loss: 0.3255 - acc: 0.8891 - val_loss: 0.3161 - val_acc: 0.8910\n",
            "Epoch 12/100\n",
            "2713/2713 [==============================] - 1s 510us/step - loss: 0.3162 - acc: 0.8935 - val_loss: 0.2810 - val_acc: 0.9013\n",
            "Epoch 13/100\n",
            "2713/2713 [==============================] - 1s 508us/step - loss: 0.2998 - acc: 0.8916 - val_loss: 0.2600 - val_acc: 0.9146\n",
            "Epoch 14/100\n",
            "2713/2713 [==============================] - 1s 513us/step - loss: 0.2803 - acc: 0.9053 - val_loss: 0.2860 - val_acc: 0.8925\n",
            "Epoch 15/100\n",
            "2713/2713 [==============================] - 1s 511us/step - loss: 0.2847 - acc: 0.8990 - val_loss: 0.2592 - val_acc: 0.9190\n",
            "Epoch 16/100\n",
            "2713/2713 [==============================] - 1s 512us/step - loss: 0.2646 - acc: 0.9126 - val_loss: 0.2577 - val_acc: 0.9234\n",
            "Epoch 17/100\n",
            "2713/2713 [==============================] - 1s 497us/step - loss: 0.2592 - acc: 0.9112 - val_loss: 0.2394 - val_acc: 0.9219\n",
            "Epoch 18/100\n",
            "2713/2713 [==============================] - 1s 425us/step - loss: 0.2441 - acc: 0.9097 - val_loss: 0.2267 - val_acc: 0.9234\n",
            "Epoch 19/100\n",
            "2713/2713 [==============================] - 1s 424us/step - loss: 0.2388 - acc: 0.9196 - val_loss: 0.2517 - val_acc: 0.9219\n",
            "Epoch 20/100\n",
            "2713/2713 [==============================] - 1s 427us/step - loss: 0.2301 - acc: 0.9208 - val_loss: 0.2252 - val_acc: 0.9161\n",
            "Epoch 21/100\n",
            "2713/2713 [==============================] - 1s 425us/step - loss: 0.2215 - acc: 0.9230 - val_loss: 0.2311 - val_acc: 0.9278\n",
            "Epoch 22/100\n",
            "2713/2713 [==============================] - 1s 425us/step - loss: 0.2093 - acc: 0.9285 - val_loss: 0.2489 - val_acc: 0.9175\n",
            "Epoch 23/100\n",
            "2713/2713 [==============================] - 1s 440us/step - loss: 0.2106 - acc: 0.9285 - val_loss: 0.2080 - val_acc: 0.9396\n",
            "Epoch 24/100\n",
            "2713/2713 [==============================] - 1s 482us/step - loss: 0.2068 - acc: 0.9241 - val_loss: 0.2240 - val_acc: 0.9205\n",
            "Epoch 25/100\n",
            "2713/2713 [==============================] - 1s 489us/step - loss: 0.2170 - acc: 0.9222 - val_loss: 0.2359 - val_acc: 0.9234\n",
            "Epoch 26/100\n",
            "2713/2713 [==============================] - 1s 487us/step - loss: 0.1970 - acc: 0.9285 - val_loss: 0.2298 - val_acc: 0.9249\n",
            "Epoch 27/100\n",
            "2713/2713 [==============================] - 1s 486us/step - loss: 0.1947 - acc: 0.9266 - val_loss: 0.2933 - val_acc: 0.9087\n",
            "Epoch 28/100\n",
            "2713/2713 [==============================] - 1s 486us/step - loss: 0.1901 - acc: 0.9307 - val_loss: 0.2505 - val_acc: 0.9205\n",
            "Epoch 29/100\n",
            "2713/2713 [==============================] - 1s 486us/step - loss: 0.1777 - acc: 0.9403 - val_loss: 0.2042 - val_acc: 0.9278\n",
            "Epoch 30/100\n",
            "2713/2713 [==============================] - 1s 484us/step - loss: 0.1799 - acc: 0.9351 - val_loss: 0.2195 - val_acc: 0.9234\n",
            "Epoch 31/100\n",
            "2713/2713 [==============================] - 1s 445us/step - loss: 0.1894 - acc: 0.9296 - val_loss: 0.1993 - val_acc: 0.9352\n",
            "Epoch 32/100\n",
            "2713/2713 [==============================] - 1s 428us/step - loss: 0.1773 - acc: 0.9333 - val_loss: 0.2838 - val_acc: 0.9087\n",
            "Epoch 33/100\n",
            "2713/2713 [==============================] - 1s 420us/step - loss: 0.1626 - acc: 0.9425 - val_loss: 0.2040 - val_acc: 0.9352\n",
            "Epoch 34/100\n",
            "2713/2713 [==============================] - 1s 426us/step - loss: 0.1684 - acc: 0.9418 - val_loss: 0.1909 - val_acc: 0.9337\n",
            "Epoch 35/100\n",
            "2713/2713 [==============================] - 1s 425us/step - loss: 0.1549 - acc: 0.9462 - val_loss: 0.1968 - val_acc: 0.9323\n",
            "Epoch 36/100\n",
            "2713/2713 [==============================] - 1s 428us/step - loss: 0.1604 - acc: 0.9443 - val_loss: 0.2055 - val_acc: 0.9323\n",
            "Epoch 37/100\n",
            "2713/2713 [==============================] - 1s 429us/step - loss: 0.1560 - acc: 0.9436 - val_loss: 0.1959 - val_acc: 0.9352\n",
            "Epoch 38/100\n",
            "2713/2713 [==============================] - 1s 424us/step - loss: 0.1521 - acc: 0.9440 - val_loss: 0.2200 - val_acc: 0.9278\n",
            "Epoch 39/100\n",
            "2713/2713 [==============================] - 1s 431us/step - loss: 0.1643 - acc: 0.9403 - val_loss: 0.2136 - val_acc: 0.9278\n",
            "Epoch 40/100\n",
            "2713/2713 [==============================] - 1s 420us/step - loss: 0.1466 - acc: 0.9466 - val_loss: 0.2403 - val_acc: 0.9278\n",
            "Epoch 41/100\n",
            "2713/2713 [==============================] - 1s 426us/step - loss: 0.1376 - acc: 0.9502 - val_loss: 0.1888 - val_acc: 0.9323\n",
            "Epoch 42/100\n",
            "2713/2713 [==============================] - 1s 425us/step - loss: 0.1450 - acc: 0.9532 - val_loss: 0.1875 - val_acc: 0.9293\n",
            "Epoch 43/100\n",
            "2713/2713 [==============================] - 1s 431us/step - loss: 0.1605 - acc: 0.9410 - val_loss: 0.2327 - val_acc: 0.9264\n",
            "Epoch 44/100\n",
            "2713/2713 [==============================] - 1s 428us/step - loss: 0.1480 - acc: 0.9484 - val_loss: 0.2201 - val_acc: 0.9323\n",
            "Epoch 45/100\n",
            "2713/2713 [==============================] - 1s 431us/step - loss: 0.1362 - acc: 0.9506 - val_loss: 0.2169 - val_acc: 0.9219\n",
            "Epoch 46/100\n",
            "2713/2713 [==============================] - 1s 425us/step - loss: 0.1330 - acc: 0.9517 - val_loss: 0.1897 - val_acc: 0.9352\n",
            "Epoch 47/100\n",
            "2713/2713 [==============================] - 1s 425us/step - loss: 0.1445 - acc: 0.9469 - val_loss: 0.1855 - val_acc: 0.9352\n",
            "Epoch 48/100\n",
            "2713/2713 [==============================] - 1s 425us/step - loss: 0.1267 - acc: 0.9583 - val_loss: 0.2371 - val_acc: 0.9219\n",
            "Epoch 49/100\n",
            "2713/2713 [==============================] - 1s 429us/step - loss: 0.1225 - acc: 0.9543 - val_loss: 0.1971 - val_acc: 0.9249\n",
            "Epoch 50/100\n",
            "2713/2713 [==============================] - 1s 429us/step - loss: 0.1247 - acc: 0.9543 - val_loss: 0.1885 - val_acc: 0.9367\n",
            "Epoch 51/100\n",
            "2713/2713 [==============================] - 1s 423us/step - loss: 0.1248 - acc: 0.9569 - val_loss: 0.1718 - val_acc: 0.9367\n",
            "Epoch 52/100\n",
            "2713/2713 [==============================] - 1s 428us/step - loss: 0.1463 - acc: 0.9480 - val_loss: 0.1825 - val_acc: 0.9396\n",
            "Epoch 53/100\n",
            "2713/2713 [==============================] - 1s 423us/step - loss: 0.1125 - acc: 0.9628 - val_loss: 0.2598 - val_acc: 0.9205\n",
            "Epoch 54/100\n",
            "2713/2713 [==============================] - 1s 424us/step - loss: 0.1294 - acc: 0.9495 - val_loss: 0.2048 - val_acc: 0.9396\n",
            "Epoch 55/100\n",
            "2713/2713 [==============================] - 1s 422us/step - loss: 0.1214 - acc: 0.9547 - val_loss: 0.2367 - val_acc: 0.9161\n",
            "Epoch 56/100\n",
            "2713/2713 [==============================] - 1s 423us/step - loss: 0.1191 - acc: 0.9583 - val_loss: 0.2060 - val_acc: 0.9323\n",
            "Epoch 57/100\n",
            "2713/2713 [==============================] - 1s 423us/step - loss: 0.1236 - acc: 0.9532 - val_loss: 0.2357 - val_acc: 0.9396\n",
            "Epoch 58/100\n",
            "2713/2713 [==============================] - 1s 422us/step - loss: 0.1174 - acc: 0.9591 - val_loss: 0.2577 - val_acc: 0.9190\n",
            "Epoch 59/100\n",
            "2713/2713 [==============================] - 1s 423us/step - loss: 0.1128 - acc: 0.9558 - val_loss: 0.2435 - val_acc: 0.9205\n",
            "Epoch 60/100\n",
            "2713/2713 [==============================] - 1s 429us/step - loss: 0.1244 - acc: 0.9565 - val_loss: 0.2053 - val_acc: 0.9367\n",
            "Epoch 61/100\n",
            "2713/2713 [==============================] - 1s 422us/step - loss: 0.1130 - acc: 0.9606 - val_loss: 0.2206 - val_acc: 0.9249\n",
            "Epoch 62/100\n",
            "2713/2713 [==============================] - 1s 426us/step - loss: 0.1179 - acc: 0.9580 - val_loss: 0.2428 - val_acc: 0.9175\n",
            "Epoch 63/100\n",
            "2713/2713 [==============================] - 1s 425us/step - loss: 0.1201 - acc: 0.9532 - val_loss: 0.2180 - val_acc: 0.9264\n",
            "Epoch 64/100\n",
            "2713/2713 [==============================] - 1s 419us/step - loss: 0.1138 - acc: 0.9576 - val_loss: 0.2120 - val_acc: 0.9381\n",
            "Epoch 65/100\n",
            "2713/2713 [==============================] - 1s 431us/step - loss: 0.1109 - acc: 0.9580 - val_loss: 0.1947 - val_acc: 0.9337\n",
            "Epoch 66/100\n",
            "2713/2713 [==============================] - 1s 430us/step - loss: 0.1044 - acc: 0.9602 - val_loss: 0.2006 - val_acc: 0.9455\n",
            "Epoch 67/100\n",
            "2713/2713 [==============================] - 1s 422us/step - loss: 0.1003 - acc: 0.9617 - val_loss: 0.2292 - val_acc: 0.9278\n",
            "Epoch 68/100\n",
            "2713/2713 [==============================] - 1s 426us/step - loss: 0.1077 - acc: 0.9565 - val_loss: 0.2244 - val_acc: 0.9352\n",
            "Epoch 69/100\n",
            "2713/2713 [==============================] - 1s 423us/step - loss: 0.1069 - acc: 0.9606 - val_loss: 0.2370 - val_acc: 0.9278\n",
            "Epoch 70/100\n",
            "2713/2713 [==============================] - 1s 422us/step - loss: 0.1156 - acc: 0.9576 - val_loss: 0.1646 - val_acc: 0.9440\n",
            "Epoch 71/100\n",
            "2713/2713 [==============================] - 1s 423us/step - loss: 0.0920 - acc: 0.9701 - val_loss: 0.2027 - val_acc: 0.9396\n",
            "Epoch 72/100\n",
            "2713/2713 [==============================] - 1s 423us/step - loss: 0.1069 - acc: 0.9613 - val_loss: 0.1954 - val_acc: 0.9352\n",
            "Epoch 73/100\n",
            "2713/2713 [==============================] - 1s 424us/step - loss: 0.1012 - acc: 0.9617 - val_loss: 0.2353 - val_acc: 0.9352\n",
            "Epoch 74/100\n",
            "2713/2713 [==============================] - 1s 422us/step - loss: 0.1053 - acc: 0.9639 - val_loss: 0.2132 - val_acc: 0.9278\n",
            "Epoch 75/100\n",
            "2713/2713 [==============================] - 1s 424us/step - loss: 0.1428 - acc: 0.9521 - val_loss: 0.2545 - val_acc: 0.9278\n",
            "Epoch 76/100\n",
            "2713/2713 [==============================] - 1s 431us/step - loss: 0.1030 - acc: 0.9646 - val_loss: 0.1864 - val_acc: 0.9426\n",
            "Epoch 77/100\n",
            "2713/2713 [==============================] - 1s 426us/step - loss: 0.0970 - acc: 0.9654 - val_loss: 0.2369 - val_acc: 0.9293\n",
            "Epoch 78/100\n",
            "2713/2713 [==============================] - 1s 419us/step - loss: 0.0946 - acc: 0.9654 - val_loss: 0.2403 - val_acc: 0.9337\n",
            "Epoch 79/100\n",
            "2713/2713 [==============================] - 1s 423us/step - loss: 0.0972 - acc: 0.9620 - val_loss: 0.2041 - val_acc: 0.9396\n",
            "Epoch 80/100\n",
            "2713/2713 [==============================] - 1s 424us/step - loss: 0.1023 - acc: 0.9624 - val_loss: 0.2306 - val_acc: 0.9367\n",
            "Epoch 81/100\n",
            "2713/2713 [==============================] - 1s 424us/step - loss: 0.0827 - acc: 0.9716 - val_loss: 0.2396 - val_acc: 0.9293\n",
            "Epoch 82/100\n",
            "2713/2713 [==============================] - 1s 429us/step - loss: 0.0938 - acc: 0.9661 - val_loss: 0.1849 - val_acc: 0.9485\n",
            "Epoch 83/100\n",
            "2713/2713 [==============================] - 1s 429us/step - loss: 0.0946 - acc: 0.9620 - val_loss: 0.1939 - val_acc: 0.9426\n",
            "Epoch 84/100\n",
            "2713/2713 [==============================] - 1s 421us/step - loss: 0.0788 - acc: 0.9672 - val_loss: 0.2180 - val_acc: 0.9337\n",
            "Epoch 85/100\n",
            "2713/2713 [==============================] - 1s 422us/step - loss: 0.0926 - acc: 0.9654 - val_loss: 0.2238 - val_acc: 0.9381\n",
            "Epoch 86/100\n",
            "2713/2713 [==============================] - 1s 422us/step - loss: 0.1064 - acc: 0.9591 - val_loss: 0.1841 - val_acc: 0.9367\n",
            "Epoch 87/100\n",
            "2713/2713 [==============================] - 1s 424us/step - loss: 0.1032 - acc: 0.9591 - val_loss: 0.1959 - val_acc: 0.9381\n",
            "Epoch 88/100\n",
            "2713/2713 [==============================] - 1s 422us/step - loss: 0.0808 - acc: 0.9709 - val_loss: 0.2497 - val_acc: 0.9278\n",
            "Epoch 89/100\n",
            "2713/2713 [==============================] - 1s 427us/step - loss: 0.0899 - acc: 0.9683 - val_loss: 0.2243 - val_acc: 0.9411\n",
            "Epoch 90/100\n",
            "2713/2713 [==============================] - 1s 424us/step - loss: 0.0880 - acc: 0.9665 - val_loss: 0.2149 - val_acc: 0.9411\n",
            "Epoch 91/100\n",
            "2713/2713 [==============================] - 1s 430us/step - loss: 0.1057 - acc: 0.9635 - val_loss: 0.2024 - val_acc: 0.9352\n",
            "Epoch 92/100\n",
            "2713/2713 [==============================] - 1s 466us/step - loss: 0.0927 - acc: 0.9639 - val_loss: 0.2436 - val_acc: 0.9396\n",
            "Epoch 93/100\n",
            "2713/2713 [==============================] - 1s 483us/step - loss: 0.0991 - acc: 0.9635 - val_loss: 0.1748 - val_acc: 0.9411\n",
            "Epoch 94/100\n",
            "2713/2713 [==============================] - 1s 479us/step - loss: 0.0783 - acc: 0.9690 - val_loss: 0.2066 - val_acc: 0.9381\n",
            "Epoch 95/100\n",
            "2713/2713 [==============================] - 1s 484us/step - loss: 0.0968 - acc: 0.9620 - val_loss: 0.2341 - val_acc: 0.9396\n",
            "Epoch 96/100\n",
            "2713/2713 [==============================] - 1s 484us/step - loss: 0.0825 - acc: 0.9687 - val_loss: 0.2180 - val_acc: 0.9367\n",
            "Epoch 97/100\n",
            "2713/2713 [==============================] - 1s 476us/step - loss: 0.0824 - acc: 0.9720 - val_loss: 0.2265 - val_acc: 0.9411\n",
            "Epoch 98/100\n",
            "2713/2713 [==============================] - 1s 478us/step - loss: 0.0888 - acc: 0.9672 - val_loss: 0.2374 - val_acc: 0.9323\n",
            "Epoch 99/100\n",
            "2713/2713 [==============================] - 1s 484us/step - loss: 0.0914 - acc: 0.9646 - val_loss: 0.2448 - val_acc: 0.9293\n",
            "Epoch 100/100\n",
            "2713/2713 [==============================] - 1s 427us/step - loss: 0.0839 - acc: 0.9705 - val_loss: 0.2206 - val_acc: 0.9411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "or-fUx1LLtaR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "joXXY7O3L-fz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "452747f0-b67c-4c9c-b2bf-1ebbf3197197"
      },
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3392, 50, 50, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "kyQvLRyTPfLJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "a149eaca-f93a-441c-ae88-2540c2f16171"
      },
      "cell_type": "code",
      "source": [
        "i=0\n",
        "plt.imshow(x[i])\n",
        "print(y[i])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vprcijzyPlVT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = np.array([\n",
        "              [1,2,3],\n",
        "              [4 ,5 ,6],\n",
        "              [7,8,9],\n",
        "              [10,11,12]\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ym8MO7oCP8V9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9d46cc79-971d-41f4-979c-ba17312cd053"
      },
      "cell_type": "code",
      "source": [
        "a"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3],\n",
              "       [ 4,  5,  6],\n",
              "       [ 7,  8,  9],\n",
              "       [10, 11, 12]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "2q0H1WTVP_3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c580058f-dd70-4346-822f-24e67274bf81"
      },
      "cell_type": "code",
      "source": [
        "a.shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "PxkHFEwAQENC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2cb0e7df-a959-48a5-db4a-9d0aafffb75b"
      },
      "cell_type": "code",
      "source": [
        "(a == np.array([1,2,3])).T"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True, False, False, False],\n",
              "       [ True, False, False, False],\n",
              "       [ True, False, False, False]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "kgORZofKQGay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "487f878b-2169-4ea6-cc58-f31530349bc1"
      },
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3392, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "vZju8JHEQYk5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_bin = np.amin((y==np.array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Ck4bBMSQfYt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "86e03955-f54e-466e-8ff6-b6f2f789cb6f"
      },
      "cell_type": "code",
      "source": [
        "i=18\n",
        "plt.imshow(x[i])\n",
        "print(y_bin[i])"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvlmF/h//FEPZ8yBANAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "-hCVAmk6RcAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3451
        },
        "outputId": "1f7a478d-904f-4e4b-9ff5-c66bf38e9bb7"
      },
      "cell_type": "code",
      "source": [
        "'''This script goes along the blog post\n",
        "\"Building powerful image classification models using very little data\"\n",
        "from blog.keras.io.\n",
        "It uses data that can be downloaded at:\n",
        "https://www.kaggle.com/c/dogs-vs-cats/data\n",
        "In our setup, we:\n",
        "- created a data/ folder\n",
        "- created train/ and validation/ subfolders inside data/\n",
        "- created cats/ and dogs/ subfolders inside train/ and validation/\n",
        "- put the cat pictures index 0-999 in data/train/cats\n",
        "- put the cat pictures index 1000-1400 in data/validation/cats\n",
        "- put the dogs pictures index 12500-13499 in data/train/dogs\n",
        "- put the dog pictures index 13500-13900 in data/validation/dogs\n",
        "So that we have 1000 training examples for each class, and 400 validation examples for each class.\n",
        "In summary, this is our directory structure:\n",
        "```\n",
        "data/\n",
        "    train/\n",
        "        dogs/\n",
        "            dog001.jpg\n",
        "            dog002.jpg\n",
        "            ...\n",
        "        cats/\n",
        "            cat001.jpg\n",
        "            cat002.jpg\n",
        "            ...\n",
        "    validation/\n",
        "        dogs/\n",
        "            dog001.jpg\n",
        "            dog002.jpg\n",
        "            ...\n",
        "        cats/\n",
        "            cat001.jpg\n",
        "            cat002.jpg\n",
        "            ...\n",
        "```\n",
        "'''\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from keras import applications\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 8\n",
        "\n",
        "\n",
        "def save_bottlebeck_features_bin(x,y):\n",
        "\n",
        "    # build the VGG16 network\n",
        "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
        "    bottleneck_features_train = model.predict(\n",
        "        x)\n",
        "    print(bottleneck_features_train.shape)\n",
        "    np.save(open('./cChess/Weights/bottleneck_features_train_bin.npy', 'wb'),\n",
        "            bottleneck_features_train)\n",
        "    np.save(open('./cChess/Weights/bottleneck_labels_bin.npy', 'wb'),\n",
        "            y)\n",
        "\n",
        "def train_top_model_bin():\n",
        "    train_data = np.load(open('./cChess/Weights/bottleneck_features_train_bin.npy', \"rb\"))\n",
        "    train_labels = np.load(open('./cChess/Weights/bottleneck_labels_bin.npy',\"rb\"))\n",
        "\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_labels,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_split=.2)\n",
        "    model.save_weights(\"./cChess/Weights/bottleneck512_bin.h5\")\n",
        "\n",
        "\n",
        "save_bottlebeck_features_bin(x,y_bin)\n",
        "train_top_model_bin()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3392, 1, 1, 512)\n",
            "Train on 2713 samples, validate on 679 samples\n",
            "Epoch 1/100\n",
            "2713/2713 [==============================] - 2s 808us/step - loss: 0.3079 - acc: 0.8625 - val_loss: 0.1480 - val_acc: 0.9647\n",
            "Epoch 2/100\n",
            "2713/2713 [==============================] - 1s 458us/step - loss: 0.1622 - acc: 0.9362 - val_loss: 0.1254 - val_acc: 0.9588\n",
            "Epoch 3/100\n",
            "2713/2713 [==============================] - 1s 457us/step - loss: 0.1140 - acc: 0.9576 - val_loss: 0.0861 - val_acc: 0.9588\n",
            "Epoch 4/100\n",
            "2713/2713 [==============================] - 1s 446us/step - loss: 0.1044 - acc: 0.9613 - val_loss: 0.1028 - val_acc: 0.9543\n",
            "Epoch 5/100\n",
            "2713/2713 [==============================] - 1s 448us/step - loss: 0.0996 - acc: 0.9598 - val_loss: 0.0438 - val_acc: 0.9912\n",
            "Epoch 6/100\n",
            "2713/2713 [==============================] - 1s 456us/step - loss: 0.0630 - acc: 0.9801 - val_loss: 0.0309 - val_acc: 0.9926\n",
            "Epoch 7/100\n",
            "2713/2713 [==============================] - 1s 446us/step - loss: 0.0588 - acc: 0.9816 - val_loss: 0.0292 - val_acc: 0.9941\n",
            "Epoch 8/100\n",
            "2713/2713 [==============================] - 1s 450us/step - loss: 0.0709 - acc: 0.9731 - val_loss: 0.0443 - val_acc: 0.9867\n",
            "Epoch 9/100\n",
            "2713/2713 [==============================] - 1s 446us/step - loss: 0.0608 - acc: 0.9797 - val_loss: 0.0359 - val_acc: 0.9941\n",
            "Epoch 10/100\n",
            "2713/2713 [==============================] - 1s 445us/step - loss: 0.0607 - acc: 0.9760 - val_loss: 0.0657 - val_acc: 0.9823\n",
            "Epoch 11/100\n",
            "2713/2713 [==============================] - 1s 452us/step - loss: 0.0579 - acc: 0.9771 - val_loss: 0.0228 - val_acc: 0.9926\n",
            "Epoch 12/100\n",
            "2713/2713 [==============================] - 1s 459us/step - loss: 0.0484 - acc: 0.9808 - val_loss: 0.0209 - val_acc: 0.9926\n",
            "Epoch 13/100\n",
            "2713/2713 [==============================] - 1s 517us/step - loss: 0.0377 - acc: 0.9878 - val_loss: 0.0180 - val_acc: 0.9941\n",
            "Epoch 14/100\n",
            "2713/2713 [==============================] - 1s 516us/step - loss: 0.0395 - acc: 0.9856 - val_loss: 0.0181 - val_acc: 0.9941\n",
            "Epoch 15/100\n",
            "2713/2713 [==============================] - 1s 516us/step - loss: 0.0338 - acc: 0.9882 - val_loss: 0.0188 - val_acc: 0.9941\n",
            "Epoch 16/100\n",
            "2713/2713 [==============================] - 1s 498us/step - loss: 0.0382 - acc: 0.9875 - val_loss: 0.0371 - val_acc: 0.9882\n",
            "Epoch 17/100\n",
            "2713/2713 [==============================] - 1s 444us/step - loss: 0.0575 - acc: 0.9797 - val_loss: 0.0203 - val_acc: 0.9941\n",
            "Epoch 18/100\n",
            "2713/2713 [==============================] - 1s 448us/step - loss: 0.0318 - acc: 0.9889 - val_loss: 0.0234 - val_acc: 0.9926\n",
            "Epoch 19/100\n",
            "2713/2713 [==============================] - 1s 453us/step - loss: 0.0294 - acc: 0.9897 - val_loss: 0.0121 - val_acc: 0.9941\n",
            "Epoch 20/100\n",
            "2713/2713 [==============================] - 1s 498us/step - loss: 0.0322 - acc: 0.9886 - val_loss: 0.0259 - val_acc: 0.9897\n",
            "Epoch 21/100\n",
            "2713/2713 [==============================] - 1s 499us/step - loss: 0.0429 - acc: 0.9882 - val_loss: 0.0134 - val_acc: 0.9941\n",
            "Epoch 22/100\n",
            "2713/2713 [==============================] - 1s 501us/step - loss: 0.0387 - acc: 0.9867 - val_loss: 0.0097 - val_acc: 0.9985\n",
            "Epoch 23/100\n",
            "2713/2713 [==============================] - 1s 502us/step - loss: 0.0335 - acc: 0.9871 - val_loss: 0.0357 - val_acc: 0.9882\n",
            "Epoch 24/100\n",
            "2713/2713 [==============================] - 1s 498us/step - loss: 0.0339 - acc: 0.9875 - val_loss: 0.0161 - val_acc: 0.9941\n",
            "Epoch 25/100\n",
            "2713/2713 [==============================] - 1s 504us/step - loss: 0.0374 - acc: 0.9867 - val_loss: 0.0200 - val_acc: 0.9912\n",
            "Epoch 26/100\n",
            "2713/2713 [==============================] - 1s 508us/step - loss: 0.0316 - acc: 0.9889 - val_loss: 0.0147 - val_acc: 0.9941\n",
            "Epoch 27/100\n",
            "2713/2713 [==============================] - 1s 474us/step - loss: 0.0280 - acc: 0.9897 - val_loss: 0.0153 - val_acc: 0.9941\n",
            "Epoch 28/100\n",
            "2713/2713 [==============================] - 1s 448us/step - loss: 0.0202 - acc: 0.9945 - val_loss: 0.0200 - val_acc: 0.9941\n",
            "Epoch 29/100\n",
            "2713/2713 [==============================] - 1s 460us/step - loss: 0.0222 - acc: 0.9941 - val_loss: 0.0296 - val_acc: 0.9867\n",
            "Epoch 30/100\n",
            "2713/2713 [==============================] - 1s 456us/step - loss: 0.0302 - acc: 0.9875 - val_loss: 0.0229 - val_acc: 0.9926\n",
            "Epoch 31/100\n",
            "2713/2713 [==============================] - 1s 458us/step - loss: 0.0265 - acc: 0.9912 - val_loss: 0.0100 - val_acc: 0.9941\n",
            "Epoch 32/100\n",
            "2713/2713 [==============================] - 1s 452us/step - loss: 0.0233 - acc: 0.9930 - val_loss: 0.0090 - val_acc: 0.9956\n",
            "Epoch 33/100\n",
            "2713/2713 [==============================] - 1s 454us/step - loss: 0.0398 - acc: 0.9849 - val_loss: 0.0117 - val_acc: 0.9941\n",
            "Epoch 34/100\n",
            "2713/2713 [==============================] - 1s 458us/step - loss: 0.0302 - acc: 0.9900 - val_loss: 0.0066 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "2713/2713 [==============================] - 1s 449us/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0098 - val_acc: 0.9956\n",
            "Epoch 36/100\n",
            "2713/2713 [==============================] - 1s 451us/step - loss: 0.0334 - acc: 0.9867 - val_loss: 0.0137 - val_acc: 0.9956\n",
            "Epoch 37/100\n",
            "2713/2713 [==============================] - 1s 450us/step - loss: 0.0182 - acc: 0.9934 - val_loss: 0.0051 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "2713/2713 [==============================] - 1s 449us/step - loss: 0.0265 - acc: 0.9908 - val_loss: 0.0087 - val_acc: 0.9971\n",
            "Epoch 39/100\n",
            "2713/2713 [==============================] - 1s 449us/step - loss: 0.0220 - acc: 0.9915 - val_loss: 0.0176 - val_acc: 0.9926\n",
            "Epoch 40/100\n",
            "2713/2713 [==============================] - 1s 447us/step - loss: 0.0214 - acc: 0.9923 - val_loss: 0.0094 - val_acc: 0.9956\n",
            "Epoch 41/100\n",
            "2713/2713 [==============================] - 1s 452us/step - loss: 0.0284 - acc: 0.9904 - val_loss: 0.0266 - val_acc: 0.9882\n",
            "Epoch 42/100\n",
            "2713/2713 [==============================] - 1s 448us/step - loss: 0.0138 - acc: 0.9956 - val_loss: 0.0121 - val_acc: 0.9941\n",
            "Epoch 43/100\n",
            "2713/2713 [==============================] - 1s 450us/step - loss: 0.0211 - acc: 0.9912 - val_loss: 0.0183 - val_acc: 0.9912\n",
            "Epoch 44/100\n",
            "2713/2713 [==============================] - 1s 449us/step - loss: 0.0322 - acc: 0.9908 - val_loss: 0.0064 - val_acc: 0.9971\n",
            "Epoch 45/100\n",
            "2713/2713 [==============================] - 1s 445us/step - loss: 0.0211 - acc: 0.9926 - val_loss: 0.0096 - val_acc: 0.9971\n",
            "Epoch 46/100\n",
            "2713/2713 [==============================] - 1s 444us/step - loss: 0.0202 - acc: 0.9923 - val_loss: 0.0146 - val_acc: 0.9926\n",
            "Epoch 47/100\n",
            "2713/2713 [==============================] - 1s 445us/step - loss: 0.0185 - acc: 0.9930 - val_loss: 0.0074 - val_acc: 0.9956\n",
            "Epoch 48/100\n",
            "2713/2713 [==============================] - 1s 449us/step - loss: 0.0182 - acc: 0.9926 - val_loss: 0.0297 - val_acc: 0.9867\n",
            "Epoch 49/100\n",
            "2713/2713 [==============================] - 1s 458us/step - loss: 0.0234 - acc: 0.9912 - val_loss: 0.0106 - val_acc: 0.9956\n",
            "Epoch 50/100\n",
            "2713/2713 [==============================] - 1s 453us/step - loss: 0.0373 - acc: 0.9867 - val_loss: 0.0079 - val_acc: 0.9956\n",
            "Epoch 51/100\n",
            "2713/2713 [==============================] - 1s 457us/step - loss: 0.0231 - acc: 0.9912 - val_loss: 0.0046 - val_acc: 0.9985\n",
            "Epoch 52/100\n",
            "2713/2713 [==============================] - 1s 440us/step - loss: 0.0243 - acc: 0.9912 - val_loss: 0.0088 - val_acc: 0.9956\n",
            "Epoch 53/100\n",
            "2713/2713 [==============================] - 1s 438us/step - loss: 0.0179 - acc: 0.9934 - val_loss: 0.0063 - val_acc: 0.9956\n",
            "Epoch 54/100\n",
            "2713/2713 [==============================] - 1s 443us/step - loss: 0.0184 - acc: 0.9937 - val_loss: 0.0061 - val_acc: 0.9956\n",
            "Epoch 55/100\n",
            "2713/2713 [==============================] - 1s 449us/step - loss: 0.0169 - acc: 0.9937 - val_loss: 0.0089 - val_acc: 0.9956\n",
            "Epoch 56/100\n",
            "2713/2713 [==============================] - 1s 442us/step - loss: 0.0185 - acc: 0.9941 - val_loss: 0.0397 - val_acc: 0.9838\n",
            "Epoch 57/100\n",
            "2713/2713 [==============================] - 1s 444us/step - loss: 0.0294 - acc: 0.9878 - val_loss: 0.0202 - val_acc: 0.9912\n",
            "Epoch 58/100\n",
            "2713/2713 [==============================] - 1s 446us/step - loss: 0.0221 - acc: 0.9893 - val_loss: 0.0058 - val_acc: 0.9971\n",
            "Epoch 59/100\n",
            "2713/2713 [==============================] - 1s 458us/step - loss: 0.0209 - acc: 0.9930 - val_loss: 0.0177 - val_acc: 0.9926\n",
            "Epoch 60/100\n",
            "2713/2713 [==============================] - 1s 454us/step - loss: 0.0175 - acc: 0.9941 - val_loss: 0.0246 - val_acc: 0.9912\n",
            "Epoch 61/100\n",
            "2713/2713 [==============================] - 1s 445us/step - loss: 0.0146 - acc: 0.9952 - val_loss: 0.0110 - val_acc: 0.9985\n",
            "Epoch 62/100\n",
            "2713/2713 [==============================] - 1s 448us/step - loss: 0.0258 - acc: 0.9926 - val_loss: 0.0243 - val_acc: 0.9882\n",
            "Epoch 63/100\n",
            "2713/2713 [==============================] - 1s 440us/step - loss: 0.0115 - acc: 0.9952 - val_loss: 0.0079 - val_acc: 0.9956\n",
            "Epoch 64/100\n",
            "2713/2713 [==============================] - 1s 446us/step - loss: 0.0086 - acc: 0.9974 - val_loss: 0.0037 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "2713/2713 [==============================] - 1s 450us/step - loss: 0.0217 - acc: 0.9915 - val_loss: 0.0038 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "2713/2713 [==============================] - 1s 444us/step - loss: 0.0212 - acc: 0.9919 - val_loss: 0.0139 - val_acc: 0.9941\n",
            "Epoch 67/100\n",
            "2713/2713 [==============================] - 1s 442us/step - loss: 0.0187 - acc: 0.9930 - val_loss: 0.0246 - val_acc: 0.9912\n",
            "Epoch 68/100\n",
            "2713/2713 [==============================] - 1s 448us/step - loss: 0.0246 - acc: 0.9934 - val_loss: 0.0159 - val_acc: 0.9941\n",
            "Epoch 69/100\n",
            "2713/2713 [==============================] - 1s 445us/step - loss: 0.0190 - acc: 0.9919 - val_loss: 0.0121 - val_acc: 0.9941\n",
            "Epoch 70/100\n",
            "2713/2713 [==============================] - 1s 438us/step - loss: 0.0212 - acc: 0.9923 - val_loss: 0.0109 - val_acc: 0.9941\n",
            "Epoch 71/100\n",
            "2713/2713 [==============================] - 1s 447us/step - loss: 0.0222 - acc: 0.9930 - val_loss: 0.0069 - val_acc: 0.9971\n",
            "Epoch 72/100\n",
            "2713/2713 [==============================] - 1s 444us/step - loss: 0.0192 - acc: 0.9934 - val_loss: 0.0104 - val_acc: 0.9956\n",
            "Epoch 73/100\n",
            "2713/2713 [==============================] - 1s 446us/step - loss: 0.0100 - acc: 0.9948 - val_loss: 0.0136 - val_acc: 0.9941\n",
            "Epoch 74/100\n",
            "2713/2713 [==============================] - 1s 444us/step - loss: 0.0223 - acc: 0.9915 - val_loss: 0.0059 - val_acc: 0.9985\n",
            "Epoch 75/100\n",
            "2713/2713 [==============================] - 1s 445us/step - loss: 0.0156 - acc: 0.9952 - val_loss: 0.0222 - val_acc: 0.9941\n",
            "Epoch 76/100\n",
            "2713/2713 [==============================] - 1s 458us/step - loss: 0.0224 - acc: 0.9923 - val_loss: 0.0049 - val_acc: 0.9985\n",
            "Epoch 77/100\n",
            "2713/2713 [==============================] - 1s 454us/step - loss: 0.0118 - acc: 0.9952 - val_loss: 0.0044 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "2713/2713 [==============================] - 1s 447us/step - loss: 0.0194 - acc: 0.9912 - val_loss: 0.0040 - val_acc: 0.9985\n",
            "Epoch 79/100\n",
            "2713/2713 [==============================] - 1s 442us/step - loss: 0.0112 - acc: 0.9948 - val_loss: 0.0078 - val_acc: 0.9956\n",
            "Epoch 80/100\n",
            "2713/2713 [==============================] - 1s 442us/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.0031 - val_acc: 0.9985\n",
            "Epoch 81/100\n",
            "2713/2713 [==============================] - 1s 448us/step - loss: 0.0077 - acc: 0.9974 - val_loss: 0.0037 - val_acc: 0.9985\n",
            "Epoch 82/100\n",
            "2713/2713 [==============================] - 1s 448us/step - loss: 0.0165 - acc: 0.9952 - val_loss: 0.0045 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "2713/2713 [==============================] - 1s 454us/step - loss: 0.0168 - acc: 0.9948 - val_loss: 0.0088 - val_acc: 0.9956\n",
            "Epoch 84/100\n",
            "2713/2713 [==============================] - 1s 441us/step - loss: 0.0136 - acc: 0.9937 - val_loss: 0.0043 - val_acc: 0.9985\n",
            "Epoch 85/100\n",
            "2713/2713 [==============================] - 1s 482us/step - loss: 0.0237 - acc: 0.9904 - val_loss: 0.0050 - val_acc: 0.9956\n",
            "Epoch 86/100\n",
            "2713/2713 [==============================] - 1s 504us/step - loss: 0.0139 - acc: 0.9956 - val_loss: 0.0163 - val_acc: 0.9941\n",
            "Epoch 87/100\n",
            "2713/2713 [==============================] - 1s 500us/step - loss: 0.0118 - acc: 0.9941 - val_loss: 0.0054 - val_acc: 0.9956\n",
            "Epoch 88/100\n",
            "2713/2713 [==============================] - 1s 499us/step - loss: 0.0191 - acc: 0.9937 - val_loss: 0.0069 - val_acc: 0.9956\n",
            "Epoch 89/100\n",
            "2713/2713 [==============================] - 1s 505us/step - loss: 0.0102 - acc: 0.9952 - val_loss: 0.0036 - val_acc: 0.9985\n",
            "Epoch 90/100\n",
            "2713/2713 [==============================] - 1s 498us/step - loss: 0.0260 - acc: 0.9908 - val_loss: 0.0234 - val_acc: 0.9926\n",
            "Epoch 91/100\n",
            "2713/2713 [==============================] - 1s 502us/step - loss: 0.0098 - acc: 0.9974 - val_loss: 0.0038 - val_acc: 0.9985\n",
            "Epoch 92/100\n",
            "2713/2713 [==============================] - 1s 489us/step - loss: 0.0126 - acc: 0.9948 - val_loss: 0.0028 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "2713/2713 [==============================] - 1s 443us/step - loss: 0.0134 - acc: 0.9956 - val_loss: 0.0047 - val_acc: 0.9956\n",
            "Epoch 94/100\n",
            "2713/2713 [==============================] - 1s 441us/step - loss: 0.0121 - acc: 0.9967 - val_loss: 0.0164 - val_acc: 0.9941\n",
            "Epoch 95/100\n",
            "2713/2713 [==============================] - 1s 440us/step - loss: 0.0090 - acc: 0.9974 - val_loss: 0.0031 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "2713/2713 [==============================] - 1s 440us/step - loss: 0.0154 - acc: 0.9941 - val_loss: 0.0026 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "2713/2713 [==============================] - 1s 443us/step - loss: 0.0139 - acc: 0.9945 - val_loss: 0.0014 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "2713/2713 [==============================] - 1s 458us/step - loss: 0.0077 - acc: 0.9974 - val_loss: 0.0087 - val_acc: 0.9985\n",
            "Epoch 99/100\n",
            "2713/2713 [==============================] - 1s 452us/step - loss: 0.0198 - acc: 0.9934 - val_loss: 0.0137 - val_acc: 0.9941\n",
            "Epoch 100/100\n",
            "2713/2713 [==============================] - 1s 439us/step - loss: 0.0161 - acc: 0.9945 - val_loss: 0.0143 - val_acc: 0.9926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5EA2V7acR3QD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ec7dd911-90f4-4c7d-c001-79f87f09c4e8"
      },
      "cell_type": "code",
      "source": [
        "!cd cChess && git status"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31mWeights/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9gErUzU0ScJi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd cChess && git add ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nhH1fAJeSehm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c6fd33cf-51b1-4495-e9a0-d17d344f0e1e"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[master 2bcfc09] trying some convnets architectures (94% accuracy on 13-classes classification and 99-100% accuracy on blank-piece classification on validation data)\n",
            " 6 files changed, 0 insertions(+), 0 deletions(-)\n",
            " create mode 100644 Weights/bottleneck512.h5\n",
            " create mode 100644 Weights/bottleneck512_bin.h5\n",
            " create mode 100644 Weights/bottleneck_features_train.npy\n",
            " create mode 100644 Weights/bottleneck_features_train_bin.npy\n",
            " create mode 100644 Weights/bottleneck_labels.npy\n",
            " create mode 100644 Weights/bottleneck_labels_bin.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tcnRMTWDS1va",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd cChess && git config --global user.email \"afonso.delgado@hotmail.com\"\n",
        "!cd cChess && git config --global user.name \"Afonso Delgado\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lfEvpGh9TG6Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd cChess && git commit -m \"trying some convnets architectures (94% accuracy on 13-classes classification and 99-100% accuracy on blank-piece classification on validation data)\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qvkzr57KTLWU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1a286888-da99-47fa-ce57-3cc460e4e5d5"
      },
      "cell_type": "code",
      "source": [
        "!cd cChess && git push"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting objects: 8, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (8/8), done.\n",
            "Writing objects: 100% (8/8), 3.78 MiB | 4.71 MiB/s, done.\n",
            "Total 8 (delta 1), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To https://github.com/rafaelmcam/cChess.git\n",
            "   e5ca9ad..2bcfc09  master -> master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "--8vBFGeTOAb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}